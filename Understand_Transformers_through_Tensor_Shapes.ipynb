{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Understand Transformers through Tensor Shapes**\n",
        "\n",
        "**Philosophy**: dimensions help tell the story. If we understand the tensor shapes, we understand the data flow; and if we understand the data flow, we understand the architecture. If we can trace the journey of an input vector from $d_\\text{model} \\rightarrow d_\\text{head} \\rightarrow d_\\text{model}$ for example, then we have a good grasp of the transformer."
      ],
      "metadata": {
        "id": "hrIgGQoj_o4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project features**\n",
        "- Step-by-step analysis of a decoder-only transformer that emphasizes dimensions of key tensors and tie-ins to the big picture, to provide deeper understanding through implementation.\n",
        "- A built-from-scratch GPT-2 based on [ARENA's code](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter1_transformer_interp/exercises/part1_transformer_from_scratch/1.1_Transformer_from_Scratch_exercises.ipynb?t=20250915), with a clean and \"overly commented\" implementation to walk through details component-by-component.\n",
        "\n",
        "**Prerequisites:** I'm assuming reader comfort with Python, machine learning and neural networks, and matrix multiplication, as well as some familiarity with PyTorch and transformer basics.\n",
        "\n",
        "**How to use this notebook**\n",
        "- To get a unique high-level perspective, read \"Overview: GPT-2 Architecture,\" specifically the subsection \"Attention: a Tensor Shapes View.\"\n",
        "- For a computation refresher/reference (particularly with `einops` calculations), skip to \"Code: GPT-2 Implementation,\" and refer to specific code snippets/comments.\n",
        "- For a full tutorial, read this document in order: get the high level overview of the architecture, and then go through the code.\n",
        "\n",
        "This notebook started from my personal notes while self-studying [ARENA's AI Safety Curriculum](https://www.arena.education/), specifically the [Transformer from Scratch chapter](https://arena-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch) (which I highly recommend). I wanted to create a rigorous reference for myself and I hope this is helpful for others as well."
      ],
      "metadata": {
        "id": "o2-Bi9Rr_jLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://i.imgur.com/mmCSFQv.png\" width=\"250\">\n",
        "\n",
        "(Thanks Dall-E)\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "no9FvSQ1Ems6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview: GPT-2 Architecture\n",
        "\n"
      ],
      "metadata": {
        "id": "fqoyB294XyDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Master Blueprint\n",
        "\n",
        "This is what we'll implement from scratch in the code below!\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-new2.png\">\n",
        "\n",
        "Image from [ARENA's Transformer from Scratch notebook](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter1_transformer_interp/exercises/part1_transformer_from_scratch/1.1_Transformer_from_Scratch_exercises.ipynb?t=20250915)\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "7U7wKZu5D8ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers in a Nutshell\n",
        "\n",
        "**The residual stream**\n",
        "- The residual stream is the shared memory flowing through each layer of the transformer.\n",
        "- The embedded input is the first state of the residual stream. Each layer in the transformer (either an attention layer or multi-layer perceptron) reads from the stream by taking a copy of the stream as input, and then adds information back into the stream.\n",
        "\n",
        "**Attention layers specialize and move information**\n",
        "- Attention layers are the communication hub for the residual stream.\n",
        "- Specialization: each attention head reads a copy of the stream and projects it to a lower-dimensional subspace, which forces the head to extract specific features.\n",
        "- Moving information: each head's attention mechanism \"looks\" across the entire sequence and copies info from relevant context tokens into each token's representation.\n",
        "- Writing to memory: these parallel insights are projected and summed simultaneously, and then added to the residual stream.\n",
        "\n",
        "**Multi-layer Perceptron (MLP) layers expand and process information**\n",
        "- Expansion: each layer projects a copy of the residual stream into a higher-dimensional hidden space, which allows the model to map the gathered features to its internal knowledge.\n",
        "- Process: each layer \"thinks\" by performing complex non-linear computations (like activations).\n",
        "- Writing to memory: the result is compressed back to the residual stream dimension and then added to the residual stream."
      ],
      "metadata": {
        "id": "UdFum5tORkCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⏩ Attention: a Tensor Shapes View\n",
        "\n",
        "**About this section**\n",
        "- This section goes through the critical computations used in multi-head self-attention, with a focus on tensor shapes to understand the journey of the input.\n",
        "- This section parallels the implementation in the Multi-Head Self Attention code section in our `Attention` module.\n",
        "- Note that other than batch size and sequence length, axis order can differ in the code implementation compared to the figures.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zuoI_fuzKd99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q, K, V Matrices\n",
        "\n",
        "- For each head, project input from *embedding space* to lower-dimensional *\"head space\"* (representation subspace).\n",
        "- Below we show the computation for Q -- the computations for K, V are analogous.\n",
        "- $x$ is the input (the normalized residual).\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/SaX2hrG.png\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "_mdRbFyvzHzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Scores/Weights\n",
        "\n",
        "High level insight\n",
        "- For each head, compress the representation subspace into a *single scalar value*.\n",
        "- For a given sequence in the batch and given attention head, the attention score between token $i$ and $j$ determines how much information from token $j$ (the source/key) should be moved into the representation of token $i$ (the destination/query).\n",
        "\n",
        "Technical details\n",
        "- When we \"match indices,\" we preserve them as a parallel dimension -- for example, here we parallelize over the batch dimension and over each attention head.\n",
        "- When we sum over an axis (or axes), we collapse the axis/axes -- we mix indices and combine information within the axis/axes, compressing the multi-dimensional info into just a scalar.\n",
        "- Note that query length and key length are the same as sequence length -- we use difference indices for the second axis of Q and the second axis of K: \"query position\" and \"key position\" respectively, to reflect their distinct roles in the attention mechanism.\n",
        "- The actual attention weights (the result of applying causal masking and softmax to the attention scores) have the same shape as the attention scores (causal masking and softmax preserve dimensions).\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/Kb1urRY.png\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "wvpFpk1MyQV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context Vector\n",
        "\n",
        "- We fill each query token's position with a weighted average of values -- the weights are given by the attention scores, and the values are the actual content being gathered.\n",
        "\n",
        "<img src=\"https://i.imgur.com/oCYzC0K.png\">"
      ],
      "metadata": {
        "id": "EskMjFNs_MxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Output\n",
        "\n",
        "- Concatenate the outputs of all heads and then project them back into the residual stream dimension -- this projection is a weighted sum that integrates the specialized information from each head into a single update.\n",
        "\n",
        "<img src=\"https://i.imgur.com/8i0dzJl.png\">"
      ],
      "metadata": {
        "id": "K6wG5z6P_IzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensions Cheat Sheet\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/SvvrnWW.png\">\n",
        "</center>\n",
        "\n",
        "Notes:\n",
        "- Attention weights have two sequence length axes, one for the query and another for the key.\n",
        "\n"
      ],
      "metadata": {
        "id": "s2wkRVNGD4ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code: GPT-2 Implementation\n",
        "\n",
        "Annotations and additions to a built-from-scratch transformer based on [ARENA's code](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter1_transformer_interp/exercises/part1_transformer_from_scratch/1.1_Transformer_from_Scratch_exercises.ipynb?t=20250915).\n",
        "\n",
        "This recreates the decoder-only architecture of GPT-2.  "
      ],
      "metadata": {
        "id": "XzV8g1g42jwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Setup"
      ],
      "metadata": {
        "id": "ZMwbIfCHGCPp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "SzLxYinisegp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install -U transformer_lens==2.11.0 einops jaxtyping -q\n",
        "\n",
        "# Common Python utilities\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Callable\n",
        "from rich import print as rprint\n",
        "from rich.table import Table\n",
        "\n",
        "# Pytorch & related shenanigans\n",
        "import einops\n",
        "import numpy as np\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from jaxtyping import Float, Int\n",
        "\n",
        "# Let's get that progress bar!\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# TransformerLens mechanistic interpretability library\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
        "\n",
        "# Hugging Face\n",
        "import datasets\n",
        "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't fold layer norm's learned parameters into the subsequent linear layer\n",
        "#   -- not folding makes it easier to analyze the layer norm activations themselves\n",
        "# Don't center the unembedding and writing matrix\n",
        "#   -- not centering keeps the weights as they were trained in the original GPT-2 model\n",
        "#   -- note that centering is a trick to make the weights more interpretable\n",
        "\n",
        "reference_gpt2 = HookedTransformer.from_pretrained(\n",
        "    \"gpt2-small\",\n",
        "    fold_ln=False,\n",
        "    center_unembed=False,\n",
        "    center_writing_weights=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu6zRd0KiyI1",
        "outputId": "c4c96147-c62e-487b-d26b-d15f05626c26"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "LGjvFHU6Ye4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚠️ Vectorization as Parallelization\n",
        "\n",
        "In implementation, computation is parallelized across all sequences in a batch in one pass by vectorizing operations along the batch size dimension, which we make the leading dimension.\n",
        "\n",
        "This is very slick, but can make it difficult to match math equations to code.\n",
        "\n",
        "For example, the well-known dot product attention formula\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\frac{\\text{softmax}(QK^T)}{\\sqrt{d_k}} V$$\n",
        "\n",
        "assumes that $Q, K, V$ are 2D matrices. (Here, $d_k$ is equal to `d_head`, our head dimension.)\n",
        "\n",
        "In code, these matrices are not 2D. In our code, for example (see the `Attention` module), $Q, K, V$ are actually 4D (`batch_size`, `n_heads`, `seq_len`, `d_head`). This is why we use `einops`, which under-the-hood parallelizes this dot product computation for each sequence within the batch and for each attention head.\n"
      ],
      "metadata": {
        "id": "voSDaVb6SYTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence Length vs Max Context Length\n",
        "\n",
        "A note on `seq_len` vs `n_ctx`:\n",
        "- `seq_len` (sequence length) is the actual tokens the model is processing in a given batch.\n",
        "- `n_ctx` is the maximum context length (it is the architectural limit).\n",
        "\n",
        "But what about padding?\n",
        "- We can either pad inputs to the longest sequence in the batch or pad to maximum context length.\n",
        "- Padding to the longest sequence in the batch is efficient and can prevent unnecessary padding.\n",
        "- Padding to maximum context length makes sense when the training corpus is, say, long blocks of text that are divided into maximum context length chunks.\n",
        "- Using `seq_len` handles both cases.\n",
        "\n",
        "An interesting note... in the case of padding to the longest sequence in the batch, we should make sure that we see\n",
        "training examples with an original length (no padding) equal to maximum context length -- otherwise we will have untrained weights!\n",
        "- In our `PosEmbed` class below... we have `self.W_pos` as shape `(n_ctx, d_model)`\n",
        "- But we only use `self.W_pos[:seq_len]` in the forward pass.\n",
        "- So we want some batches to have `seq_len` equal to `n_ctx` so that we actually train/update all of `self.W_pos`."
      ],
      "metadata": {
        "id": "G59y9TELqSX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "sO-iMDQKk-L3"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "\n",
        "    d_model: int = 768\n",
        "    \"\"\"Token embedding dimension.\"\"\"\n",
        "\n",
        "    d_vocab: int = 50257\n",
        "    \"\"\"Number of tokens in vocabulary.\"\"\"\n",
        "\n",
        "    n_ctx: int = 1024\n",
        "    \"\"\"Maximum input length in number of tokens.\"\"\"\n",
        "\n",
        "    d_head: int = 64\n",
        "    \"\"\"Token embedding dimension per attention head.\n",
        "    d_model = d_head x n_heads\"\"\"\n",
        "\n",
        "    d_mlp: int = 3072\n",
        "    \"\"\"Dimension of hidden layer in the MLP block.\"\"\"\n",
        "\n",
        "    n_heads: int = 12\n",
        "    \"\"\"Number of attention heads.\"\"\"\n",
        "\n",
        "    n_layers: int = 12\n",
        "    \"\"\"Number of transformer blocks.\"\"\"\n",
        "\n",
        "    layer_norm_eps: float = 1e-5\n",
        "    \"\"\"Small constant added to prevent division by zero in layer norm.\"\"\"\n",
        "\n",
        "    init_std: float = 0.02\n",
        "    \"\"\"Standard deviation of normal random variable for initializing weights.\"\"\"\n",
        "\n",
        "    debug: bool = False\n",
        "    \"\"\"A toggle to help the user (not used explicitly in this section).\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ],
      "metadata": {
        "id": "0HtdA5VsYx0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(nn.Module):\n",
        "    \"\"\"\n",
        "    Convert tokens to embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_E, std=self.cfg.init_std)\n",
        "\n",
        "    def forward(\n",
        "        self, tokens: Int[Tensor, \"batch_size seq_len\"]\n",
        "    ) -> Float[Tensor, \"batch_size seq_len d_model\"]:\n",
        "\n",
        "        # This is mathematically equivalent to O x W_E\n",
        "        #   where O is the one-hot encoded version of the input\n",
        "        #   --> this is differentiable!\n",
        "        return self.W_E[tokens]"
      ],
      "metadata": {
        "id": "6v1XSNebui7m"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Position Embedding"
      ],
      "metadata": {
        "id": "P0EJsDF9Y4zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalization"
      ],
      "metadata": {
        "id": "DDcDqhqYYoCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    For each input sequence in the batch, for each token in that\n",
        "    input sequence, normalize the token across its embedding dimension.\n",
        "\n",
        "    Features (\"hidden units\") of a given token all share the same\n",
        "    normalization terms.\n",
        "\n",
        "    But each token has token-specific normalization terms.\n",
        "\n",
        "    Source: https://www.cs.utoronto.ca/~hinton/absps/LayerNormalization.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
        "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
        "\n",
        "    def forward(\n",
        "        self, residual: Float[Tensor, \"batch_size seq_len d_model\"]\n",
        "    ) -> Float[Tensor, \"batch_size seq_len d_model\"]:\n",
        "\n",
        "        # Both variables are shape (batch_size, seq_len, 1) -- we keepdim for correct broadcasting\n",
        "        residual_mean = residual.mean(dim=2, keepdim=True)\n",
        "        residual_stdev = t.sqrt(t.var(residual, dim=2, unbiased=False, keepdim=True) + self.cfg.layer_norm_eps)\n",
        "\n",
        "        return (residual - residual_mean)/residual_stdev * self.w + self.b"
      ],
      "metadata": {
        "id": "vQ0GpQ_YpnRA"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PosEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    Learned positional embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_pos, std=self.cfg.init_std)\n",
        "\n",
        "    def forward(\n",
        "        self, tokens: Int[Tensor, \"batch_size seq_len\"]\n",
        "    ) -> Float[Tensor, \"batch_size seq_len d_model\"]:\n",
        "\n",
        "        batch_size, seq_len = tokens.shape\n",
        "\n",
        "        pos_embed = self.W_pos[:seq_len].unsqueeze(0) # Shape (1, seq_len, d_model)\n",
        "\n",
        "        # Repeat across the batch_size dimension\n",
        "        # The same positional embedding is added to each input sequence\n",
        "        #   in the batch\n",
        "        return pos_embed.expand(batch_size, seq_len, self.cfg.d_model)"
      ],
      "metadata": {
        "id": "f9KulpQV_Ddw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Block (Decoder)"
      ],
      "metadata": {
        "id": "eFEbcfg7ZWBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=https://substackcdn.com/image/fetch/$s_!qbpc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6133c18-bfaf-4578-8c5a-e5ac7809f65b_1632x784.png>\n",
        "\n",
        "Image from an article on decoder-only transformers by Cameron Wolfe, [link here](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse).\n",
        "</center>\n",
        "\n",
        "Computation hints\n",
        "- Attention is across-token -- in the `Attention` module below, the attention scores are the result of an `einops` operation with **two** sequence length indices in the output (`posn_Q` and `posn_K`).\n",
        "- MLP is per-token -- in the `MLP` module below, both `einops` operations in the forward pass have **one** sequence length index in the output (`seq_len`).  \n"
      ],
      "metadata": {
        "id": "m4d_kxtTWX6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "mTPw2iwuY9Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention implemented via tensor parallelism.\n",
        "\n",
        "    Individual heads are handled as a dimension in the QKV tensors --\n",
        "        we do NOT make new Attention modules for each attention head.\n",
        "\n",
        "    Source: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    # Type checking: `IGNORE` is a `Tensor`\n",
        "    # The empty string means it is a scalar\n",
        "    IGNORE: Float[Tensor, \"\"]\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Learned QKV projection matrices\n",
        "        # Shape (n_heads, d_model, d_head) -- these matrices decompose\n",
        "        #   the model into specialized heads -- they move stuff from\n",
        "        #   the model dimension to the head dimension\n",
        "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "\n",
        "        # Writing output\n",
        "        # Note: these are different dimensions than the QKV counterparts\n",
        "        # The shape of W_O is NOT a typo here! Shape (n_heads, d_head, d_model) --\n",
        "        #   this matrix reassembles the heads back into the main stream --\n",
        "        #   it moves stuff from each head's head dimension to the model dimension\n",
        "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
        "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
        "\n",
        "        # Learned QKV biases\n",
        "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
        "\n",
        "        # We want to \"break the symmetry\" and initialize weights\n",
        "        #   to different (random) values -- if they all have the\n",
        "        #   same value, all their gradients will be the same,\n",
        "        #   and then network cannot learn\n",
        "        # Note that the biases are initialized to zero -- this is\n",
        "        #   standard practice\n",
        "        # Source: https://www.baeldung.com/cs/ml-neural-network-weights\n",
        "        nn.init.normal_(self.W_Q, std=self.cfg.init_std)\n",
        "        nn.init.normal_(self.W_K, std=self.cfg.init_std)\n",
        "        nn.init.normal_(self.W_V, std=self.cfg.init_std)\n",
        "        nn.init.normal_(self.W_O, std=self.cfg.init_std)\n",
        "\n",
        "        # Buffers are tensors that are part of the model's state\n",
        "        #   but NOT updated during backprop\n",
        "        # We use this as a mask -- we mask attention scores by\n",
        "        #   setting masked values to -inf so that after softmax,\n",
        "        #   we still have valid probabilities\n",
        "        self.register_buffer(\"IGNORE\",\n",
        "                             t.tensor(float(\"-inf\"), dtype=t.float32, device=device))\n",
        "\n",
        "    def forward(\n",
        "        self, normalized_resid_pre: Float[Tensor, \"batch_size seq_len d_model\"]\n",
        "    ) -> Float[Tensor, \"batch_size seq_len d_model\"]:\n",
        "\n",
        "        # Move from a dense representation (embedding) to a multi-head view\n",
        "        # Linear projection from d_model to d_head across all heads simultaneously\n",
        "        # Each head sees a unique low-rank version of the input\n",
        "        # q, k, v are all shape (batch_size, seq_len, n_heads, d_head)\n",
        "        qkv_proj_einops_command = \"batch seq d_model, n_heads d_model d_head -> batch seq n_heads d_head\"\n",
        "        Q = einops.einsum(normalized_resid_pre, self.W_Q, qkv_proj_einops_command) + self.b_Q\n",
        "        K = einops.einsum(normalized_resid_pre, self.W_K, qkv_proj_einops_command) + self.b_K\n",
        "        V = einops.einsum(normalized_resid_pre, self.W_V, qkv_proj_einops_command) + self.b_V\n",
        "\n",
        "        # Compute the attention scores\n",
        "        # For each head, in that head's dimension (subspace), take the dot product\n",
        "        #   of each query and each key and store that pairwise value\n",
        "        # Note: posn_Q = posn_K = seq_len -- we just use different names/indices so\n",
        "        #   that einsum does not collapse/reduce them -- since posn_Q and posn_K are\n",
        "        #   in the einsum output, we get all the pairwise attention scores between\n",
        "        #   token i in the query and token j in the key\n",
        "        attn_einops_command = \"batch posn_Q n_heads d_head, batch posn_K n_heads d_head -> batch n_heads posn_Q posn_K\"\n",
        "        attn_scores = einops.einsum(Q,\n",
        "                                    K,\n",
        "                                    attn_einops_command) # attention scores: raw\n",
        "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head**0.5)\n",
        "\n",
        "        attn_pattern = attn_scores_masked.softmax(-1) # attention pattern/weights: after softmax\n",
        "\n",
        "        # Fill each query token's position with a weighted average of values,\n",
        "        #   the attention pattern (query-key match) defines the weights.\n",
        "        # Can think of this as moving info from source to destination.\n",
        "        # Context vector (values that are queried and aggregated according to attention scores)\n",
        "        # Context vector has shape (batch_size, seq_len, n_heads, d_head)\n",
        "        z_einops_command = \"batch n_heads posn_Q posn_K, batch posn_K n_heads d_head -> batch posn_Q n_heads d_head\"\n",
        "        z = einops.einsum(attn_pattern,\n",
        "                          V,\n",
        "                          z_einops_command)\n",
        "\n",
        "        # Calculate output -- project each head's output back to model dimension and\n",
        "        #   sum the outputs of heads (sum over n_heads and d_head dimension)\n",
        "        # This gives the concatenated multi-head attention output, with shape (batch_size, seq_len, d_model)\n",
        "        attn_out_einops_command = \"batch posn_Q n_heads d_head, n_heads d_head d_model -> batch posn_Q d_model\"\n",
        "        attn_out = einops.einsum(z,\n",
        "                                 self.W_O,\n",
        "                                 attn_out_einops_command) + self.b_O\n",
        "\n",
        "        # Note that the above just a more elegant version of the following\n",
        "        #\n",
        "        # Concatenate heads -- shape (batch_size, seq_len, n_heads * d_head)\n",
        "        # Flatten W_O to match -- shape (n_heads * d_head, d_model)\n",
        "        # Project back to residual stream dimension\n",
        "        #\n",
        "        # z_concatenated = einops.rearrange(z, \"batch posn_Q n_heads d_head -> batch pos_Q (n_heads d_head)\")\n",
        "        # W_O_flattened = einops.rearrange(self.W_O, \"n_heads d_head d_model -> (n_heads d_head) d_model\")\n",
        "        # attn_out = z_concatenated @ W_O_flattened\n",
        "\n",
        "        return attn_out\n",
        "\n",
        "    def apply_causal_mask(\n",
        "        self,\n",
        "        attn_scores: Float[Tensor, \"batch_size n_heads posn_Q posn_K\"],\n",
        "    ) -> Float[Tensor, \"batch_size n_heads query_pos key_pos\"]:\n",
        "        \"\"\"\n",
        "        By causal masking, we mean that we are turning this model into a predictor\n",
        "            (not just a pattern matcher). Ensures next-token prediction only\n",
        "            depends on previous tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        # all_ones is shape (posn_Q, posn_K)\n",
        "        all_ones = t.ones(attn_scores.size(-2),\n",
        "                          attn_scores.size(-1),\n",
        "                          device=attn_scores.device)\n",
        "\n",
        "        # Don't mask the diagonal -- each token is allowed to look at itself\n",
        "        mask = t.triu(all_ones, diagonal=1).bool()\n",
        "\n",
        "        # Again, we parallelize the batch_size and n_heads dimension\n",
        "        # For each batch and head attention head, we set the above-diagonal\n",
        "        #   part of the attention scores matrix to -inf (so that the transformer\n",
        "        #   is not allowed to \"look ahead\" at future tokens)\n",
        "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
        "\n",
        "        return attn_scores"
      ],
      "metadata": {
        "id": "N33jexC14WVc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed Forward Neural Network\n",
        "\n",
        "Multi-layer perceptron (MLP) or Feed Forward Neural Network (FFNN)."
      ],
      "metadata": {
        "id": "-IF6G3GVZB-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard MLP -- for each token (in each sequence in the batch),\n",
        "        apply a fully connected layer (expanding the input to the hidden\n",
        "        dimension d_mlp), GeLU, and then another fully connected layer\n",
        "        (return the input to the original embedding dimension d_model).\n",
        "\n",
        "    Remember: the MLP applies to EACH TOKEN INDEPENDENTLY!\n",
        "        The MLP is a pointwise/position-wise operation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Notice the dimensions are transposed for \"in\" vs \"out\"\n",
        "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
        "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
        "\n",
        "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
        "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
        "\n",
        "        nn.init.normal_(self.W_in, std=self.cfg.init_std)\n",
        "        nn.init.normal_(self.W_out, std=self.cfg.init_std)\n",
        "\n",
        "    def forward(\n",
        "        self, normalized_resid_mid: Float[Tensor, \"batch_size seq_len d_model\"]\n",
        "    ) -> Float[Tensor, \"batch_size seq_len d_model\"]:\n",
        "\n",
        "        # Expansion phase\n",
        "        # Pre-activation fully connected linear layer output with\n",
        "        #   shape (batch_size, seq_len, d_mlp) -- expanding the feature space to\n",
        "        #   a larger MLP hidden state\n",
        "        pre = einops.einsum(normalized_resid_mid,\n",
        "                            self.W_in,\n",
        "                            \"batch seq d_model, d_model d_mlp -> batch seq d_mlp\") + self.b_in\n",
        "\n",
        "        # Use GPT2's GeLU function\n",
        "        # Source: https://github.com/TransformerLensOrg/TransformerLens/blob/main/transformer_lens/utils.py\n",
        "        post = gelu_new(pre)\n",
        "\n",
        "        # Down-projection phase\n",
        "        # Another fully connected linear layer output, with\n",
        "        #   shape (batch_size, seq_len, d_model) -- projecting back to the model dimension\n",
        "        mlp_out = einops.einsum(post,\n",
        "                                self.W_out,\n",
        "                                \"batch seq d_mlp, d_mlp d_model -> batch seq d_model\") + self.b_out\n",
        "\n",
        "        return mlp_out"
      ],
      "metadata": {
        "id": "CN55PiddOXy0"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Block Integration"
      ],
      "metadata": {
        "id": "YovAG7N1JT2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Remember that each sequence is processed in parallel within a batch.\n",
        "\n",
        "    Computations per input sequence (after tokenization and positional embedding):\n",
        "    - Normalize each token relative to its own features\n",
        "    - Get new context computed using attention\n",
        "    - Apply residual connection: add attention context and original input together\n",
        "    - Normalize this sum again -- normalize each transformed token relative to its own features\n",
        "    - Apply MLP to each transformed token\n",
        "    - Apply residual connection: add MLP output and current residual stream\n",
        "        (sum of attention context and original input)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.ln1 = LayerNorm(cfg)\n",
        "        self.attn = Attention(cfg)\n",
        "        self.ln2 = LayerNorm(cfg)\n",
        "        self.mlp = MLP(cfg)\n",
        "\n",
        "    def forward(\n",
        "        self, resid_pre: Float[Tensor, \"batch_size seq_len d_model\"]\n",
        "    ) -> Float[Tensor, \"batch_size seq_len d_model\"]:\n",
        "\n",
        "        # Each tensor here is shape (batch_size, seq_len, d_model)\n",
        "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
        "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
        "        return resid_post"
      ],
      "metadata": {
        "id": "bN5CyPzwJOem"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unembedding"
      ],
      "metadata": {
        "id": "PozG_MdegCdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Unembed(nn.Module):\n",
        "    \"\"\"\n",
        "    Convert next-word predictions from embedding space to vocab space.\n",
        "    Each token in the vocabulary gets a logit (unnormalized), where larger logit\n",
        "        values correspond to higher probabilities of that token being the next one.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
        "\n",
        "        # GPT-2 does not have bias for unembedding step\n",
        "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
        "\n",
        "        nn.init.normal_(self.W_U, std=self.cfg.init_std)\n",
        "\n",
        "    def forward(\n",
        "        self, normalized_resid_final: Float[Tensor, \"batch_size seq_len d_model\"]\n",
        "    ) -> Float[Tensor, \"batch_size seq_len d_vocab\"]:\n",
        "        \"\"\"\n",
        "        Compute logits for each token.\n",
        "        \"\"\"\n",
        "\n",
        "        return einops.einsum(normalized_resid_final,\n",
        "                             self.W_U,\n",
        "                             \"batch_size seq_len d_model, d_model d_vocab -> batch_size seq_len d_vocab\") + self.b_U"
      ],
      "metadata": {
        "id": "alSpv228eChk"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚡ Demo Transformer!"
      ],
      "metadata": {
        "id": "m59nOb32gJd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DemoTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Putting it all together :)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.embed = Embed(cfg)\n",
        "        self.pos_embed = PosEmbed(cfg)\n",
        "\n",
        "        # A regular list does not work here -- pytorch will not register the layers\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
        "\n",
        "        self.ln_final = LayerNorm(cfg)\n",
        "        self.unembed = Unembed(cfg)\n",
        "\n",
        "    def forward(\n",
        "        self, tokens: Int[Tensor, \"batch_size seq_len\"]\n",
        "    ) -> Float[Tensor, \"batch_size seq_len d_vocab\"]:\n",
        "\n",
        "        # Initialize residual stream with embedding of original input plus position info\n",
        "        # (batch_size, seq_len) -> (batch_size, seq_len, d_model)\n",
        "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
        "\n",
        "        # For each layer (transformer block), add to the residual stream\n",
        "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
        "        for block in self.blocks:\n",
        "            residual = block(residual)\n",
        "\n",
        "        # Clean-up: GPT-2 uses a final layer normalization to stabilize\n",
        "        #   the residual stream before projecting it back to the vocabulary space\n",
        "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len)\n",
        "        logits = self.unembed(self.ln_final(residual))\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "mWNm3fiWgI10"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log Probabilities"
      ],
      "metadata": {
        "id": "qEEukPqjvDCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formula Derivation\n",
        "\n",
        "\n",
        "If $Q$ is our model's distribution and $P$ is the true distribution, then the total [cross-entropy of $Q$ relative to $P$](https://en.wikipedia.org/wiki/Cross-entropy) is\n",
        "\n",
        "$$- \\sum_{x \\in V} \\sum_{c \\in C} P(x \\mid c) \\cdot \\text{log } Q(x \\mid c),$$\n",
        "where $C$ includes all possible \"contexts\" (sequences of tokens, say, up to a certain length), $x$ is the \"next token,\" and $V$ is the vocabulary.\n",
        "\n",
        "To turn this into a training loss function that we use in practice, we assume the distribution of the training data (the empirical distribution) is the true distribution, so that $P(x \\mid \\text{context})$ is simply $1$ if $x$ is the observed (true) next token in the data after the given context and $0$ otherwise.\n",
        "\n",
        "Then the formula becomes\n",
        "\n",
        "$$ -\\frac{1}{N} \\sum_{c \\in \\text{training data}} \\text{log } Q\\left(x^*(\\text{c}) \\mid \\text{c}\\right),$$\n",
        "\n",
        "where $N$ is the total number of tokens in the training set and $x^*(c)$ is the observed (true) next token in the data after the sequence of tokens $c$. This is the average negative log predicted probability!\n",
        "\n",
        "In practice, we compute the average over a mini-batch instead of the whole training set.\n",
        "\n",
        "This shows that next-token prediction training is essentially a classification problem! For each sequence of tokens, we are predicting an integer (from $1$ to $\\lvert V \\rvert$) (corresponding to the token in the vocabulary that is the next token).\n",
        "\n",
        "The above formulation is not something that is typically written -- it's more common to find simplified notation. But this formulation it makes clear the assumptions we need to use cross entropy loss, and why we use negative log probability of prediction (i.e. negative log likelihood) interchangeably with cross entropy. Many resources treat cross entropy loss as just another \"black box\" for LLMs, but exploring more detailed notation clarifies the bridge between sequence modeling and classification.\n",
        "\n",
        "For additional resources: a five minute primer on cross entropy loss by Adian Liusie [here](https://www.youtube.com/watch?v=Pwgpl9mKars) and another explanation of the cross entropy and negative log probability equivalence by Sebastian Raschka [here](https://sebastianraschka.com/blog/2022/losses-learned-part1.html)."
      ],
      "metadata": {
        "id": "3J6j0jGBGIUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_log_probs(\n",
        "    logits: Float[Tensor, \"batch_size seq_len d_vocab\"],\n",
        "    tokens: Int[Tensor, \"batch_size seq_len\"]\n",
        ") -> Float[Tensor, \"batch_size seq_len-1\"]:\n",
        "    \"\"\"\n",
        "    Negative log probability (negative log likelihood) is equivalent\n",
        "        to cross entropy (under the assumption that the observed/empirical\n",
        "        training distribution is the true distribution).\n",
        "\n",
        "    This function computes the model's estimated/predicted log probability\n",
        "        for each true next token.\n",
        "    \"\"\"\n",
        "\n",
        "    # Take the log softmax over the vocabulary dimension\n",
        "    log_probs = logits.log_softmax(dim=-1) # shape (batch_size, seq_len, d_vocab)\n",
        "\n",
        "    # The input has length seq_len, so we'll do seq_len-1 next-token predictions\n",
        "    #   to compare the model's predictions of the input to the actual input\n",
        "    # For the last token in the sequence, there is no \"next token\",\n",
        "    #   so don't include the last token\n",
        "    log_probs_pred = log_probs[:, :-1]\n",
        "\n",
        "    # These are the true/actual next tokens\n",
        "    # Note we need to unsqueeze so that the multi-dimensional lookup indexing works\n",
        "    true_labels = tokens[:, 1:].unsqueeze(-1) # shape (batch_size, seq_len-1, 1)\n",
        "\n",
        "    # Gather is multi-dimensional lookup\n",
        "    # For each true/observed token, get the model's estimated/predicted\n",
        "    #   log probability for that token\n",
        "    log_probs_for_tokens = log_probs_pred.gather(dim=-1, index=true_labels).squeeze(-1)\n",
        "\n",
        "    return log_probs_for_tokens\n"
      ],
      "metadata": {
        "id": "IEXO_YFIvIev"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation\n",
        "\n",
        "Let's verify that our demo transformer successfully recreates GPT-2."
      ],
      "metadata": {
        "id": "RzUG64AGJFcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our demo transformer and GPT-2 as a reference\n",
        "\n",
        "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "\n",
        "demo = DemoTransformer(Config()).to(device)\n",
        "demo.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
        "demo.eval()\n",
        "\n",
        "reference_gpt2.eval()\n",
        "\n",
        "# Don't auto-print the last object\n",
        "pass"
      ],
      "metadata": {
        "id": "KZiqQYwMZaCe"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@t.no_grad()\n",
        "def compare_models(prompt, demo_model, ref_model, steps=20):\n",
        "    \"\"\"\n",
        "    Validates the demo model by comparing its top-1 predictions\n",
        "    against the reference model step-by-step.\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = ref_model.to_tokens(prompt).to(device)\n",
        "\n",
        "    # Table header\n",
        "    print(f\"\\nComparing models on prompt: '{prompt}' \\n\")\n",
        "    print(f\"{'Step':<5} | {'Demo Prediction':<20} | {'Reference Prediction':<20} | {'Match?'}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for i in range(steps):\n",
        "\n",
        "        demo_logits = demo_model(tokens)\n",
        "        ref_logits, ref_cache = ref_model.run_with_cache(tokens)\n",
        "\n",
        "        # Very last token's logits\n",
        "        demo_next = demo_logits[0, -1].argmax(dim=-1)\n",
        "        ref_next = ref_logits[0, -1].argmax(dim=-1)\n",
        "\n",
        "        # Decode for display and check match\n",
        "        demo_str = ref_model.to_string(demo_next)\n",
        "        ref_str = ref_model.to_string(ref_next)\n",
        "        match = \"✅\" if demo_next == ref_next else \"❌\"\n",
        "\n",
        "        print(f\"{i+1:<5} | {repr(demo_str):<20} | {repr(ref_str):<20} | {match}\")\n",
        "\n",
        "        # Teacher forcing: append the REFERENCE token to the sequence\n",
        "        # This ensures that even if the demo misses once, we see if it recovers\n",
        "        tokens = t.cat([tokens, ref_next.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
        "\n",
        "    # Check total logit divergence is small\n",
        "    diff = (demo_logits - ref_logits).abs().max().item()\n",
        "    print(f\"\\nFinal Max Logit Difference: {diff:.6f}\")"
      ],
      "metadata": {
        "id": "IjKApLqVbJEv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models(\"Large language models are interesting because \", demo, reference_gpt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67YEBl-uhrVf",
        "outputId": "e198f995-6000-420a-8d3e-23dab10e458b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparing models on prompt: 'Large language models are interesting because ' \n",
            "\n",
            "Step  | Demo Prediction      | Reference Prediction | Match?\n",
            "-----------------------------------------------------------------\n",
            "1     | '\\xa0'               | '\\xa0'               | ✅\n",
            "2     | 'they'               | 'they'               | ✅\n",
            "3     | ' allow'             | ' allow'             | ✅\n",
            "4     | ' us'                | ' us'                | ✅\n",
            "5     | ' to'                | ' to'                | ✅\n",
            "6     | ' explore'           | ' explore'           | ✅\n",
            "7     | ' the'               | ' the'               | ✅\n",
            "8     | ' relationship'      | ' relationship'      | ✅\n",
            "9     | ' between'           | ' between'           | ✅\n",
            "10    | ' language'          | ' language'          | ✅\n",
            "11    | ' and'               | ' and'               | ✅\n",
            "12    | ' cognition'         | ' cognition'         | ✅\n",
            "13    | '.'                  | '.'                  | ✅\n",
            "14    | ' '                  | ' '                  | ✅\n",
            "15    | '\\xa0'               | '\\xa0'               | ✅\n",
            "16    | 'In'                 | 'In'                 | ✅\n",
            "17    | ' this'              | ' this'              | ✅\n",
            "18    | ' paper'             | ' paper'             | ✅\n",
            "19    | ','                  | ','                  | ✅\n",
            "20    | ' we'                | ' we'                | ✅\n",
            "\n",
            "Final Max Logit Difference: 0.000107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedily generate next tokens with our demo transformer\n",
        "\n",
        "test_string = \"I hope you enjoyed this tutorial. \"\n",
        "test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
        "\n",
        "for _ in range(20):\n",
        "    with t.no_grad():\n",
        "        logits = demo(test_tokens)\n",
        "        next_token = logits[0, -1].argmax(dim=-1).unsqueeze(0).unsqueeze(0)\n",
        "        test_tokens = t.cat([test_tokens, next_token], dim=-1)\n",
        "\n",
        "print(f\"\\nDemo Generation: {reference_gpt2.to_string(test_tokens[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDSBMEf3h4eP",
        "outputId": "a5d9ac77-061e-4b1c-ae87-fdc811eb1edb"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Demo Generation: <|endoftext|>I hope you enjoyed this tutorial.  I hope you enjoyed reading it.  I hope you enjoyed reading my blog.  \n"
          ]
        }
      ]
    }
  ]
}